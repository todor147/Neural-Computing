{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS4287 Neural Computing - Assignment 2: Convolutional Neural Networks\n",
        "\n",
        "**Team Members:** [INSERT NAMES AND ID NUMBERS]  \n",
        "**Student ID 1:** [INSERT]  \n",
        "**Student ID 2:** [INSERT]\n",
        "\n",
        "**Code Execution Status:** [Comment on whether the code executes to completion without errors]\n",
        "\n",
        "**Third Party Source:** [Provide link to any existing implementation used]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "1. [Imports and Setup](#imports)\n",
        "2. [Data Loading and Preprocessing](#data)\n",
        "3. [Network Architecture](#architecture)\n",
        "4. [Cost Function](#loss)\n",
        "5. [Optimizer](#optimizer)\n",
        "6. [Cross-Fold Validation](#validation)\n",
        "7. [Training and Results](#results)\n",
        "8. [Hyperparameter Analysis](#hyperparameters)\n",
        "9. [Evaluation](#evaluation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup {#imports}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep learning frameworks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Sklearn for evaluation and preprocessing\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preprocessing {#data}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Fruit Detection Dataset from Kaggle\n",
        "# Dataset: lakshaytyagi01/fruit-detection\n",
        "# Reorganized from YOLO format to classification format with 6 classes:\n",
        "# Apple, Banana, Grape, Orange, Pineapple, Watermelon\n",
        "\n",
        "DATASET_PATH = \"data/fruits_classification\"\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Load training dataset - Keras automatically creates labels from folder names\n",
        "train_dataset = keras.utils.image_dataset_from_directory(\n",
        "    f'{DATASET_PATH}/train',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = keras.utils.image_dataset_from_directory(\n",
        "    f'{DATASET_PATH}/test',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Load validation dataset\n",
        "val_dataset = keras.utils.image_dataset_from_directory(\n",
        "    f'{DATASET_PATH}/valid',\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Get class names and number of classes\n",
        "class_names = train_dataset.class_names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Fruit classes: {class_names}\")\n",
        "print(f\"Number of training batches: {tf.data.experimental.cardinality(train_dataset).numpy()}\")\n",
        "print(f\"Number of test batches: {tf.data.experimental.cardinality(test_dataset).numpy()}\")\n",
        "print(f\"Number of validation batches: {tf.data.experimental.cardinality(val_dataset).numpy()}\")\n",
        "\n",
        "# Store image shape for later use\n",
        "input_shape = (*IMAGE_SIZE, 3)\n",
        "print(f\"Input shape: {input_shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Data Visualization\n",
        "\n",
        "Visualize sample fruit images and their distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample fruits from training set\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "# Get first batch from training data\n",
        "for images, labels in train_dataset.take(1):\n",
        "    # Display 16 sample images\n",
        "    for i in range(min(16, len(images))):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))  # Display image\n",
        "        # Get class name from one-hot encoded label\n",
        "        label_idx = np.argmax(labels[i].numpy())\n",
        "        plt.title(f\"{class_names[label_idx]}\", fontsize=10)\n",
        "        plt.axis('off')\n",
        "\n",
        "plt.suptitle('Sample Fruits from Training Set', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot class distribution\n",
        "class_counts = {}\n",
        "total_samples = 0\n",
        "\n",
        "# Count samples per class\n",
        "for images, labels in train_dataset:\n",
        "    for label in labels:\n",
        "        idx = np.argmax(label)\n",
        "        class_name = class_names[idx]\n",
        "        class_counts[class_name] = class_counts.get(class_name, 0) + len(images)\n",
        "    total_samples += len(images)\n",
        "\n",
        "# Visualize distribution\n",
        "plt.figure(figsize=(14, 6))\n",
        "bars = plt.bar(class_counts.keys(), class_counts.values(), color='steelblue', edgecolor='black')\n",
        "plt.title('Distribution of Fruit Classes in Training Set', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Fruit Class', fontsize=12)\n",
        "plt.ylabel('Number of Images', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Add count labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height)}',\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClass Distribution:\")\n",
        "for class_name, count in sorted(class_counts.items()):\n",
        "    percentage = (count / sum(class_counts.values())) * 100\n",
        "    print(f\"  {class_name}: {count} images ({percentage:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. CNN Architecture - Transfer Learning with ResNet50 {#architecture}\n",
        "\n",
        "We use **Transfer Learning** with the pre-trained **ResNet50** architecture from ImageNet.\n",
        "\n",
        "**Why ResNet50?**\n",
        "- **Residual Learning**: Uses skip connections to solve vanishing gradient problem\n",
        "- **Deep Architecture**: 50 layers (deep enough for complex patterns)\n",
        "- **Pre-trained Weights**: Already learned features from ImageNet (1.4M images)\n",
        "- **Industry Standard**: Used in production systems worldwide\n",
        "\n",
        "**Transfer Learning Strategy**:\n",
        "1. Use ResNet50 as feature extractor (freeze base layers)\n",
        "2. Add custom classification head for 6 fruit classes\n",
        "3. Fine-tune if needed for better performance\n",
        "\n",
        "**Reference**: He et al. (2015) - \"Deep Residual Learning for Image Recognition\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Transfer Learning Model using ResNet50\n",
        "def build_transfer_learning_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Build a transfer learning model using pre-trained ResNet50.\n",
        "    \n",
        "    ResNet50 Architecture (He et al., 2015):\n",
        "    - 50 layers deep with residual (skip) connections\n",
        "    - Solves vanishing gradient problem through identity shortcuts\n",
        "    - Pre-trained on ImageNet (1.4M images, 1000 classes)\n",
        "    \n",
        "    Transfer Learning Strategy:\n",
        "    - Base Model: ResNet50 frozen weights (feature extractor)\n",
        "    - Custom Head: New layers for fruit classification\n",
        "    - Benefits: Faster training, better accuracy, less data needed\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load pre-trained ResNet50 (without top classification layer)\n",
        "    # Weights trained on ImageNet - contains general visual features\n",
        "    base_model = tf.keras.applications.ResNet50(\n",
        "        weights='imagenet',           # Use ImageNet pre-trained weights\n",
        "        include_top=False,             # Exclude original classification head\n",
        "        input_shape=input_shape,       # Our input: 224x224x3\n",
        "        pooling='avg'                  # Global average pooling at the end\n",
        "    )\n",
        "    \n",
        "    # Freeze base model layers - we use it as fixed feature extractor\n",
        "    # This preserves learned features from ImageNet\n",
        "    base_model.trainable = False\n",
        "    print(f\"✓ Loaded ResNet50 base model (frozen)\")\n",
        "    print(f\"  - Total layers in base: {len(base_model.layers)}\")\n",
        "    print(f\"  - Base model parameters: {base_model.count_params():,}\")\n",
        "    \n",
        "    # Build custom classification head for our 6 fruit classes\n",
        "    model = models.Sequential([\n",
        "        # Input layer\n",
        "        layers.Input(shape=input_shape, name='input_layer'),\n",
        "        \n",
        "        # Data augmentation layer (applied during training only)\n",
        "        layers.RandomFlip('horizontal', name='augmentation_flip'),\n",
        "        layers.RandomRotation(0.2, name='augmentation_rotation'),\n",
        "        layers.RandomZoom(0.1, name='augmentation_zoom'),\n",
        "        \n",
        "        # Pre-trained ResNet50 feature extractor\n",
        "        base_model,\n",
        "        \n",
        "        # Custom classification head\n",
        "        layers.BatchNormalization(name='bn_1'),       # Normalize features\n",
        "        layers.Dropout(0.3, name='dropout_1'),        # Regularization\n",
        "        \n",
        "        layers.Dense(512, activation='relu',          # Dense layer 1\n",
        "                    kernel_initializer='he_normal', name='fc_1'),\n",
        "        layers.BatchNormalization(name='bn_2'),\n",
        "        layers.Dropout(0.4, name='dropout_2'),\n",
        "        \n",
        "        layers.Dense(256, activation='relu',          # Dense layer 2\n",
        "                    kernel_initializer='he_normal', name='fc_2'),\n",
        "        layers.BatchNormalization(name='bn_3'),\n",
        "        layers.Dropout(0.3, name='dropout_3'),\n",
        "        \n",
        "        # Output layer - softmax for multi-class classification\n",
        "        layers.Dense(num_classes, activation='softmax', name='output_layer')\n",
        "    ], name='ResNet50_FruitClassifier')\n",
        "    \n",
        "    return model, base_model\n",
        "\n",
        "# Build the model\n",
        "print(\"=\" * 60)\n",
        "print(\"Building Transfer Learning Model with ResNet50...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model, base_model = build_transfer_learning_model(input_shape, num_classes)\n",
        "\n",
        "# Display model architecture\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MODEL ARCHITECTURE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "model.summary()\n",
        "\n",
        "# Count trainable vs non-trainable parameters\n",
        "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
        "non_trainable_params = sum([tf.size(w).numpy() for w in model.non_trainable_weights])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PARAMETER BREAKDOWN\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total parameters:        {model.count_params():,}\")\n",
        "print(f\"Trainable parameters:    {trainable_params:,}\")\n",
        "print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
        "print(f\"\\n✓ Using ResNet50 (frozen) + Custom Head\")\n",
        "print(f\"✓ Only training classification head ({trainable_params:,} params)\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Loss Function {#loss}\n",
        "\n",
        "**Categorical Cross-Entropy Loss**\n",
        "\n",
        "For multi-class classification with one-hot encoded labels:\n",
        "- Measures difference between predicted probability distribution and true distribution\n",
        "- Formula: L = -∑(y_true * log(y_pred))\n",
        "- Well-suited for softmax output layer\n",
        "- Provides strong gradients for faster learning\n",
        "\n",
        "**Alternatives considered:**\n",
        "- Sparse Categorical Cross-Entropy (for integer labels)\n",
        "- Focal Loss (for class imbalance - not needed here)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optimizer {#optimizer}\n",
        "\n",
        "**Adam Optimizer** (Adaptive Moment Estimation)\n",
        "\n",
        "**Why Adam?**\n",
        "- Combines momentum (moving average of gradients) and RMSProp (adaptive learning rates)\n",
        "- Automatically adjusts learning rate for each parameter\n",
        "- Works well with sparse gradients and noisy data\n",
        "- Industry standard for deep learning\n",
        "\n",
        "**Configuration:**\n",
        "- Learning rate: 0.001 (default, will be adjusted with ReduceLROnPlateau)\n",
        "- Beta1: 0.9 (momentum)\n",
        "- Beta2: 0.999 (RMSProp)\n",
        "\n",
        "**Alternatives:**\n",
        "- SGD with momentum: Slower but sometimes better final accuracy\n",
        "- RMSProp: Good but Adam is more stable\n",
        "- AdamW: Adam with weight decay (could improve regularization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model with loss function, optimizer, and metrics\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPILING MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model.compile(\n",
        "    # Adam optimizer - adaptive learning rate for each parameter\n",
        "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "    \n",
        "    # Categorical cross-entropy loss for multi-class classification\n",
        "    loss=losses.CategoricalCrossentropy(),\n",
        "    \n",
        "    # Track accuracy during training and validation\n",
        "    metrics=[\n",
        "        metrics.CategoricalAccuracy(name='accuracy'),\n",
        "        metrics.TopKCategoricalAccuracy(k=2, name='top_2_accuracy'),  # Top-2 predictions\n",
        "        metrics.Precision(name='precision'),\n",
        "        metrics.Recall(name='recall')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"✓ Model compiled successfully!\")\n",
        "print(f\"  - Optimizer: Adam (lr=0.001)\")\n",
        "print(f\"  - Loss: Categorical Cross-Entropy\")\n",
        "print(f\"  - Metrics: Accuracy, Top-2 Accuracy, Precision, Recall\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training with Callbacks {#training}\n",
        "\n",
        "**Training Strategy:**\n",
        "1. **Early Stopping**: Stop if validation loss doesn't improve for 5 epochs\n",
        "2. **ReduceLROnPlateau**: Reduce learning rate by 0.5x if loss plateaus for 3 epochs\n",
        "3. **ModelCheckpoint**: Save best model based on validation accuracy\n",
        "\n",
        "This prevents overfitting and optimizes training efficiency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup callbacks for training optimization\n",
        "callbacks_list = [\n",
        "    # Reduce learning rate when validation loss plateaus\n",
        "    # This helps model escape local minima and fine-tune\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',        # Watch validation loss\n",
        "        factor=0.5,                 # Reduce LR by 50%\n",
        "        patience=3,                 # Wait 3 epochs before reducing\n",
        "        min_lr=1e-7,               # Don't go below this\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    # Stop training early if no improvement\n",
        "    # Prevents overfitting and saves compute time\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',        # Watch validation loss\n",
        "        patience=5,                 # Stop after 5 epochs with no improvement\n",
        "        restore_best_weights=True,  # Restore best model weights\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    # Save best model during training\n",
        "    ModelCheckpoint(\n",
        "        'best_fruit_model.h5',     # Filename\n",
        "        monitor='val_accuracy',     # Watch validation accuracy\n",
        "        save_best_only=True,        # Only save if improved\n",
        "        mode='max',                 # Maximize accuracy\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  - Epochs: 25\")\n",
        "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  - Training samples: ~{tf.data.experimental.cardinality(train_dataset).numpy() * BATCH_SIZE}\")\n",
        "print(f\"  - Validation samples: ~{tf.data.experimental.cardinality(val_dataset).numpy() * BATCH_SIZE}\")\n",
        "print(f\"  - Callbacks: Early Stopping, ReduceLROnPlateau, ModelCheckpoint\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nTraining will take approximately 10-20 minutes with GPU...\\n\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,              # Training data\n",
        "    validation_data=val_dataset,  # Validation data\n",
        "    epochs=25,                   # Maximum epochs (early stopping may end sooner)\n",
        "    callbacks=callbacks_list,    # Apply our callbacks\n",
        "    verbose=1                    # Show progress bar\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ TRAINING COMPLETED!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results and Visualization {#results}\n",
        "\n",
        "Plot training history and evaluate model performance on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Training History - Accuracy and Loss Curves\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Training & Validation Accuracy\n",
        "axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, marker='o')\n",
        "axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s')\n",
        "axes[0, 0].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0, 0].legend(fontsize=11)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Training & Validation Loss\n",
        "axes[0, 1].plot(history.history['loss'], label='Training Loss', linewidth=2, marker='o', color='coral')\n",
        "axes[0, 1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, marker='s', color='red')\n",
        "axes[0, 1].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
        "axes[0, 1].legend(fontsize=11)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Precision\n",
        "axes[1, 0].plot(history.history['precision'], label='Training Precision', linewidth=2, marker='o', color='green')\n",
        "axes[1, 0].plot(history.history['val_precision'], label='Validation Precision', linewidth=2, marker='s', color='darkgreen')\n",
        "axes[1, 0].set_title('Model Precision Over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Precision', fontsize=12)\n",
        "axes[1, 0].legend(fontsize=11)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Recall\n",
        "axes[1, 1].plot(history.history['recall'], label='Training Recall', linewidth=2, marker='o', color='purple')\n",
        "axes[1, 1].plot(history.history['val_recall'], label='Validation Recall', linewidth=2, marker='s', color='indigo')\n",
        "axes[1, 1].set_title('Model Recall Over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Recall', fontsize=12)\n",
        "axes[1, 1].legend(fontsize=11)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Transfer Learning Training Results - ResNet50', fontsize=16, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL TRAINING METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Final Training Accuracy:   {history.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"Final Training Loss:       {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"Final Validation Loss:     {history.history['val_loss'][-1]:.4f}\")\n",
        "print(f\"Final Training Precision:  {history.history['precision'][-1]:.4f}\")\n",
        "print(f\"Final Validation Precision: {history.history['val_precision'][-1]:.4f}\")\n",
        "print(f\"Final Training Recall:     {history.history['recall'][-1]:.4f}\")\n",
        "print(f\"Final Validation Recall:   {history.history['val_recall'][-1]:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
