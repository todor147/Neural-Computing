# âœ… Notebook Updated to Use Transfer Learning with ResNet50!

## ðŸŽ‰ What Changed

Your notebook has been **upgraded** from a custom CNN to use **Transfer Learning with ResNet50** - a much better approach for your assignment!

---

## ðŸ“Š Key Improvements

### **Before (Custom CNN)**
- Built CNN from scratch (32 â†’ 64 â†’ 128 â†’ 256 filters)
- ~3-5 million parameters to train
- Slower training, potentially lower accuracy
- Difficulty Level: 1-2 marks

### **After (Transfer Learning with ResNet50)** âœ¨
- Uses pre-trained ResNet50 from ImageNet
- ~23 million parameters (frozen) + ~1 million trainable
- Faster training, higher accuracy expected
- **Difficulty Level: 2 marks (matches assignment rubric!)**
- Industry-standard approach

---

## ðŸ† Why This Is Better for Your Assignment

### 1. **Higher Difficulty Score (2 marks)**
From assignment spec:
> "d. 2 marks for Inception OR VGG OR **ResNet** â€“ 2 marks"

âœ… ResNet50 qualifies for 2 marks!

### 2. **Transfer Learning Discussion**
You can now discuss:
- Pre-trained weights from ImageNet
- Feature extraction vs fine-tuning
- Why transfer learning works
- Domain adaptation

### 3. **Residual Learning (Required for ResNet)**
From assignment spec:
> "For example, if opting for ResNet, a few paragraphs on Residual learning is necessary"

âœ… The notebook now includes ResNet with residual connections!

### 4. **Better Performance**
- Pre-trained on 1.4M ImageNet images
- Learns fruit features faster
- Expected accuracy: 90-95%+

### 5. **Built-in Data Augmentation**
- RandomFlip (horizontal)
- RandomRotation (Â±20%)
- RandomZoom (Â±10%)

---

## ðŸ“ Updated Notebook Structure

### **Cell 0: Header**
- Team info, student IDs
- Third-party source (add ResNet50 paper link)

### **Cell 1-7: Data Loading** (Unchanged)
- Imports
- Data loading
- Visualization
- Class distribution

### **Cell 8: Architecture Description** âœ¨ NEW
```markdown
## Transfer Learning with ResNet50

**Why ResNet50?**
- Residual Learning with skip connections
- 50 layers deep
- Pre-trained on ImageNet
- Industry standard

**Reference**: He et al. (2015) - "Deep Residual Learning for Image Recognition"
```

### **Cell 9: Model Building** âœ¨ UPDATED
```python
# Load pre-trained ResNet50
base_model = tf.keras.applications.ResNet50(
    weights='imagenet',
    include_top=False,
    pooling='avg'
)

# Freeze base model
base_model.trainable = False

# Add custom classification head
model = Sequential([
    Input,
    RandomFlip,
    RandomRotation,
    RandomZoom,
    base_model,  # Pre-trained ResNet50
    BatchNormalization,
    Dropout(0.3),
    Dense(512, relu),
    BatchNormalization,
    Dropout(0.4),
    Dense(256, relu),
    BatchNormalization,
    Dropout(0.3),
    Dense(6, softmax)  # 6 fruit classes
])
```

### **Cell 10-11: Loss & Optimizer** âœ¨ NEW
- Categorical Cross-Entropy explanation
- Adam optimizer justification

### **Cell 12: Model Compilation** âœ¨ NEW
```python
model.compile(
    optimizer=Adam(lr=0.001),
    loss=CategoricalCrossentropy(),
    metrics=['accuracy', 'precision', 'recall']
)
```

### **Cell 13-14: Training** âœ¨ NEW
```python
callbacks = [
    ReduceLROnPlateau(),
    EarlyStopping(),
    ModelCheckpoint()
]

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=25,
    callbacks=callbacks
)
```

### **Cell 15-16: Results Visualization** âœ¨ NEW
- 4-panel plot: Accuracy, Loss, Precision, Recall
- Training vs Validation curves
- Final metrics summary

### **Cells 17+: Need to Add**
You still need to add:
- Test set evaluation
- Confusion matrix
- Per-class metrics
- Sample predictions
- Hyperparameter experiments

---

## ðŸš€ How to Run Your Updated Notebook

### **Step 1: Update Third-Party Source (Cell 0)**
Add this to Cell 0:
```python
# **Third Party Source:** 
# - ResNet50: https://arxiv.org/abs/1512.03385 (He et al., 2015)
# - Keras Applications: https://keras.io/api/applications/resnet/
```

### **Step 2: Update Dataset Path for Colab (Cell 5)**
Change:
```python
DATASET_PATH = "data/fruits_classification"
```
To:
```python
# Mount Google Drive first!
from google.colab import drive
drive.mount('/content/drive')

DATASET_PATH = "/content/drive/MyDrive/CS4287_Assignment/data/fruits_classification"
```

### **Step 3: Run All Cells**
1. In Google Colab: Runtime â†’ Run all
2. Enable GPU: Runtime â†’ Change runtime type â†’ GPU
3. Wait for ResNet50 to download (~100MB)
4. Training will take ~10-20 minutes with GPU

### **Step 4: Save Results**
Add a new cell at the end:
```python
# Save trained model
model.save('/content/drive/MyDrive/CS4287_Assignment/fruit_model_resnet50.h5')

# Save training plots
plt.savefig('/content/drive/MyDrive/CS4287_Assignment/training_history.png', dpi=300)

print("âœ… All results saved to Google Drive!")
```

---

## ðŸ“š What to Write in Your PDF Report

### **Section: Network Architecture**

**Title**: "Transfer Learning with ResNet50"

**Content**:
1. **What is ResNet50?**
   - 50-layer deep convolutional neural network
   - Introduced residual (skip) connections
   - Solves vanishing gradient problem in very deep networks
   
2. **Residual Learning** (REQUIRED by assignment spec)
   - Traditional networks: y = F(x)
   - ResNet: y = F(x) + x (adds identity shortcut)
   - Allows gradients to flow directly backward
   - Enables training of very deep networks (100+ layers)
   
3. **Transfer Learning Strategy**
   - Pre-trained on ImageNet (1.4M images, 1000 classes)
   - Frozen base model (23M parameters) as feature extractor
   - Custom classification head (1M parameters) trained on fruits
   - Benefits: Faster training, less data needed, better accuracy
   
4. **Architecture Diagram**
   ```
   Input (224x224x3)
         â†“
   Data Augmentation (Flip, Rotate, Zoom)
         â†“
   ResNet50 Base (frozen)
   - Conv1 (64 filters)
   - Conv2_x (Residual blocks)
   - Conv3_x (Residual blocks)
   - Conv4_x (Residual blocks)
   - Conv5_x (Residual blocks)
   - Global Avg Pool
         â†“
   Custom Head (trainable)
   - BatchNorm â†’ Dropout(0.3)
   - Dense(512, ReLU) â†’ BatchNorm â†’ Dropout(0.4)
   - Dense(256, ReLU) â†’ BatchNorm â†’ Dropout(0.3)
   - Dense(6, Softmax)
         â†“
   Output (6 fruit classes)
   ```

5. **Hyperparameters**
   - Weight initialization: ImageNet pre-trained (base) + He Normal (head)
   - Activation: ReLU (hidden), Softmax (output)
   - Batch Normalization: After each dense layer
   - Regularization: Dropout (0.3-0.4) + Data Augmentation
   - Optimizer: Adam (lr=0.001)
   - Loss: Categorical Cross-Entropy

---

## ðŸ§ª Hyperparameter Experiments to Run

Now you need to run 2-3 experiments (REQUIRED by assignment):

### **Experiment 1: Different Learning Rates**
```python
# Try: 0.0001, 0.001 (baseline), 0.01
model.compile(
    optimizer=Adam(learning_rate=0.0001),  # Change this
    loss=CategoricalCrossentropy(),
    metrics=['accuracy']
)
```

### **Experiment 2: Fine-Tuning vs Frozen**
```python
# Unfreeze last few layers of ResNet50
base_model.trainable = True

# Freeze only first 140 layers (out of 175)
for layer in base_model.layers[:140]:
    layer.trainable = False

# Use lower learning rate for fine-tuning
model.compile(optimizer=Adam(learning_rate=0.0001), ...)
```

### **Experiment 3: Different Architectures**
```python
# Try MobileNetV2 instead of ResNet50
base_model = tf.keras.applications.MobileNetV2(
    weights='imagenet',
    include_top=False,
    pooling='avg'
)
```

### **Experiment 4: More/Less Data Augmentation**
```python
# Try more aggressive augmentation
layers.RandomFlip('horizontal_and_vertical'),  # Both directions
layers.RandomRotation(0.3),  # Â±30%
layers.RandomZoom(0.2),      # Â±20%
layers.RandomContrast(0.2),  # Add contrast
```

**For each experiment:**
- Document what changed
- Compare accuracy, loss, training time
- Analyze why it helped or hurt
- Show plots side-by-side

---

## âœ… Assignment Checklist Update

- [x] âœ… Using ResNet architecture (2 difficulty marks)
- [x] âœ… Transfer learning implemented
- [x] âœ… Can discuss residual learning (required for ResNet)
- [x] âœ… Data augmentation included
- [x] âœ… Batch normalization used
- [x] âœ… Dropout regularization
- [x] âœ… He initialization (in custom head)
- [x] âœ… Adam optimizer
- [x] âœ… Categorical cross-entropy loss
- [ ] â³ Need to add: Test evaluation
- [ ] â³ Need to add: Confusion matrix
- [ ] â³ Need to add: Hyperparameter experiments
- [ ] â³ Need to add: Cross-fold validation discussion

---

## ðŸŽ¯ Expected Results

With ResNet50 transfer learning, you should see:

- **Training Accuracy**: 95-99%
- **Validation Accuracy**: 90-95%
- **Test Accuracy**: 88-93%
- **Training Time**: 10-20 minutes (25 epochs with GPU)
- **Convergence**: Fast (5-10 epochs to reach 90%+)

If you see:
- **Overfitting**: Training acc >> Validation acc â†’ Add more dropout, augmentation
- **Underfitting**: Both acc < 85% â†’ Unfreeze more layers, train longer
- **Good fit**: Training acc â‰ˆ Validation acc (within 5%) â†’ Perfect! âœ…

---

## ðŸ“– References to Add

For your PDF report references section:

1. **He, K., Zhang, X., Ren, S., & Sun, J. (2015)**. "Deep Residual Learning for Image Recognition". arXiv:1512.03385.

2. **Deng, J., Dong, W., Socher, R., et al. (2009)**. "ImageNet: A Large-Scale Hierarchical Image Database". CVPR 2009.

3. **Yosinski, J., Clune, J., Bengio, Y., & Lipson, H. (2014)**. "How transferable are features in deep neural networks?". NIPS 2014.

4. **Keras Applications Documentation**. ResNet50. https://keras.io/api/applications/resnet/

5. **Ioffe, S., & Szegedy, C. (2015)**. "Batch Normalization: Accelerating Deep Network Training". arXiv:1502.03167.

6. **Kingma, D. P., & Ba, J. (2014)**. "Adam: A Method for Stochastic Optimization". arXiv:1412.6980.

---

## ðŸ’¡ Pro Tips for Your Report

1. **Diagram**: Draw ResNet50 architecture showing residual blocks
2. **Explain Skip Connections**: Show how y = F(x) + x works
3. **Justify Transfer Learning**: Why ImageNet features help with fruits
4. **Compare Approaches**: Custom CNN vs Transfer Learning
5. **Discuss Trade-offs**: Frozen vs Fine-tuned, Speed vs Accuracy

---

## ðŸ†˜ Common Issues & Solutions

### **Issue: ResNet50 download fails**
**Solution**: Colab will auto-download. If it fails, manually download:
```python
!wget https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5
```

### **Issue: Out of memory**
**Solution**: Reduce batch size:
```python
BATCH_SIZE = 16  # Instead of 32
```

### **Issue: Training too slow**
**Solution**: Check GPU is enabled:
```python
print(tf.config.list_physical_devices('GPU'))
# Should show GPU device
```

---

## ðŸŽ“ Grading Impact

**Before (Custom CNN)**: 
- Difficulty: 1 mark
- Expected grade: 11-15/20 (Accomplished)

**After (ResNet50 Transfer Learning)**:
- Difficulty: 2 marks
- Expected grade: 14-18/20 (Accomplished to Exemplary)
- **Why?** ResNet explicitly mentioned in rubric, transfer learning shows advanced understanding

---

## âœ¨ Next Steps

1. âœ… **Done**: Notebook updated with ResNet50
2. â³ **Now**: Run the notebook in Colab
3. â³ **Then**: Run 2-3 hyperparameter experiments
4. â³ **Finally**: Write PDF report using results

You're on track for a great submission! ðŸš€

---

**Questions?** Run the notebook and let me know if you hit any issues!

